{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"DTS.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"id":"JJdCMNCw-WIQ","executionInfo":{"status":"ok","timestamp":1627914145452,"user_tz":-480,"elapsed":9,"user":{"displayName":"Jasper Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXkbcEwWi8xY-48do5y_JlYoJ8T1RHzMTLTD80tg=s64","userId":"05397851641183407839"}}},"source":["#DTS notebook"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uICnA_9dMo-i","executionInfo":{"status":"ok","timestamp":1627914158137,"user_tz":-480,"elapsed":12693,"user":{"displayName":"Jasper Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXkbcEwWi8xY-48do5y_JlYoJ8T1RHzMTLTD80tg=s64","userId":"05397851641183407839"}},"outputId":"6f6ab9ea-3486-4d38-a78b-716cee3b85fa"},"source":["! pip install sklearn-pandas\n","! pip install pycox\n","! pip install lifelines"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: sklearn-pandas in /usr/local/lib/python3.7/dist-packages (1.8.0)\n","Requirement already satisfied: pandas>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-pandas) (1.1.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from sklearn-pandas) (1.4.1)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from sklearn-pandas) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-pandas) (0.22.2.post1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.11.0->sklearn-pandas) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.11.0->sklearn-pandas) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.11.0->sklearn-pandas) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.15.0->sklearn-pandas) (1.0.1)\n","Collecting pycox\n","  Downloading pycox-0.2.2-py3-none-any.whl (73 kB)\n","\u001b[K     |████████████████████████████████| 73 kB 1.6 MB/s \n","\u001b[?25hCollecting torchtuples>=0.2.0\n","  Downloading torchtuples-0.2.0-py3-none-any.whl (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 586 kB/s \n","\u001b[?25hCollecting py7zr>=0.11.3\n","  Downloading py7zr-0.16.1-py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pycox) (2.23.0)\n","Requirement already satisfied: scikit-learn>=0.21.2 in /usr/local/lib/python3.7/dist-packages (from pycox) (0.22.2.post1)\n","Requirement already satisfied: feather-format>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pycox) (0.4.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from pycox) (3.1.0)\n","Requirement already satisfied: numba>=0.44 in /usr/local/lib/python3.7/dist-packages (from pycox) (0.51.2)\n","Requirement already satisfied: pyarrow>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from feather-format>=0.4.0->pycox) (3.0.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->pycox) (1.19.5)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->pycox) (1.5.2)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.44->pycox) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.44->pycox) (57.2.0)\n","Collecting brotli>=1.0.9\n","  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n","\u001b[K     |████████████████████████████████| 357 kB 60.1 MB/s \n","\u001b[?25hCollecting pyzstd<0.15.0,>=0.14.4\n","  Downloading pyzstd-0.14.4-cp37-cp37m-manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 47.2 MB/s \n","\u001b[?25hCollecting texttable\n","  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n","Collecting pyppmd>=0.14.0\n","  Downloading pyppmd-0.15.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n","\u001b[K     |████████████████████████████████| 121 kB 71.4 MB/s \n","\u001b[?25hCollecting multivolumefile>=0.2.3\n","  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from py7zr>=0.11.3->pycox) (4.6.1)\n","Collecting pycryptodomex>=3.6.6\n","  Downloading pycryptodomex-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 55.4 MB/s \n","\u001b[?25hCollecting bcj-cffi<0.6.0,>=0.5.1\n","  Downloading bcj_cffi-0.5.1-cp37-cp37m-manylinux2014_x86_64.whl (36 kB)\n","Requirement already satisfied: cffi>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from bcj-cffi<0.6.0,>=0.5.1->py7zr>=0.11.3->pycox) (1.14.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.14.0->bcj-cffi<0.6.0,>=0.5.1->py7zr>=0.11.3->pycox) (2.20)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pycox) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pycox) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pycox) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pycox) (1.24.3)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->pycox) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.2->pycox) (1.0.1)\n","Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from torchtuples>=0.2.0->pycox) (1.1.5)\n","Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from torchtuples>=0.2.0->pycox) (3.2.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0.3->torchtuples>=0.2.0->pycox) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->torchtuples>=0.2.0->pycox) (2018.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->py7zr>=0.11.3->pycox) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->py7zr>=0.11.3->pycox) (3.7.4.3)\n","Installing collected packages: texttable, pyzstd, pyppmd, pycryptodomex, multivolumefile, brotli, bcj-cffi, torchtuples, py7zr, pycox\n","Successfully installed bcj-cffi-0.5.1 brotli-1.0.9 multivolumefile-0.2.3 py7zr-0.16.1 pycox-0.2.2 pycryptodomex-3.10.1 pyppmd-0.15.2 pyzstd-0.14.4 texttable-1.6.4 torchtuples-0.2.0\n","Collecting lifelines\n","  Downloading lifelines-0.26.0-py3-none-any.whl (348 kB)\n","\u001b[K     |████████████████████████████████| 348 kB 8.6 MB/s \n","\u001b[?25hCollecting formulaic<0.3,>=0.2.2\n","  Downloading formulaic-0.2.4-py3-none-any.whl (55 kB)\n","\u001b[K     |████████████████████████████████| 55 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.1.5)\n","Collecting autograd-gamma>=0.3\n","  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n","Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.3)\n","Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (3.2.2)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.4.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from lifelines) (1.19.5)\n","Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->lifelines) (0.16.0)\n","Collecting interface-meta>=1.2\n","  Downloading interface_meta-1.2.3-py2.py3-none-any.whl (14 kB)\n","Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (0.8.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from formulaic<0.3,>=0.2.2->lifelines) (1.12.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.8.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0->lifelines) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0->lifelines) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->lifelines) (2018.9)\n","Building wheels for collected packages: autograd-gamma\n","  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4048 sha256=5d141b7fc3721e759c8b9f2a1588198c46cf8d948436f0bc75a3964ab9725364\n","  Stored in directory: /root/.cache/pip/wheels/9f/01/ee/1331593abb5725ff7d8c1333aee93a50a1c29d6ddda9665c9f\n","Successfully built autograd-gamma\n","Installing collected packages: interface-meta, formulaic, autograd-gamma, lifelines\n","Successfully installed autograd-gamma-0.5.0 formulaic-0.2.4 interface-meta-1.2.3 lifelines-0.26.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0L40z5AHuIE0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626669233617,"user_tz":-480,"elapsed":29371,"user":{"displayName":"Jasper Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXkbcEwWi8xY-48do5y_JlYoJ8T1RHzMTLTD80tg=s64","userId":"05397851641183407839"}},"outputId":"1606d026-6aed-403b-da9e-e76c156abf05"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1avR1IAuMsyt","executionInfo":{"status":"ok","timestamp":1627914162091,"user_tz":-480,"elapsed":3959,"user":{"displayName":"Jasper Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXkbcEwWi8xY-48do5y_JlYoJ8T1RHzMTLTD80tg=s64","userId":"05397851641183407839"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# For preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn_pandas import DataFrameMapper \n","\n","\n","import torch # For building the networks \n","from torch import nn\n","import torch.nn.functional as F\n","import torchtuples as tt # Some useful functions\n","\n","from pycox.datasets import metabric\n","from pycox.models import LogisticHazard,CoxPH\n","from pycox.models.loss import NLLLogistiHazardLoss, CoxPHLoss, NLLPMFLoss,NLLPCHazardLoss\n","from pycox.evaluation import EvalSurv\n","\n","from lifelines.utils import concordance_index\n","from lifelines import CoxPHFitter\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","\n","np.random.seed(1234)\n","_ = torch.manual_seed(1234)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"95rW0FLMMCUg"},"source":["#Loose Integration Models "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-kk9Q7-CTC3h"},"source":["#Deepsurv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6yzRwGlXKEG"},"source":["#Experiment Deepsurv\n","\n","def sinlge_data_experiment_ds(data,dtype,maxind,dropout_rate,netsize,n_batch,resultdf,bsvec):\n","\n","  for ind in np.arange(1,maxind + 1): #go through all 20 train/test pairs\n","    np.random.seed(1234)\n","    torchseed = torch.manual_seed(1234)\n","    \n","    testind = np.array(pd.read_csv(ind_folder + 'testind' + str(ind)+ \".csv\")[\"x\"]) - 1 # minus 1 to match the array index in Python\n","    trainind = np.array(pd.read_csv(ind_folder + 'trainind' + str(ind)+ \".csv\")[\"x\"]) - 1\n","\n","    print('start with seed' + str(ind))\n","\n","    i = data.shape[1]\n","\n","    df_train = data.loc[trainind,]\n","    df_test = data.loc[testind,]\n","\n","\n","    df_train[\"age\"] = Age[trainind]\n","    df_train[\"stage\"] = Stage[trainind]\n","    df_test[\"age\"] = Age[testind]\n","    df_test[\"stage\"] = Stage[testind]\n","    if ndata != \"breast\": \n","      df_train[\"gender\"] = Gender[trainind]\n","      df_test[\"gender\"] = Gender[testind] \n","\n","    Y_train = Y[trainind]\n","    Y_test = Y[testind]\n","    E_train = E[trainind]\n","    E_test = E[testind]\n","\n","\n","    df_train[\"time\"] = Y_train.astype('float32')\n","    df_train[\"event\"] = E_train.astype('float32')\n","    df_train,df_val = train_test_split(df_train, test_size=0.2)\n","    df_test[\"time\"] = Y_test.astype('float32')\n","    df_test[\"event\"] = E_test.astype('float32')\n","\n","    #cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n","    var_num = i + 2\n","    if ndata != \"breast\": var_num = i + 3\n","    cols_leave = df_train.columns[0:var_num]\n","    print(cols_leave)\n","    #standardize = [([col], StandardScaler()) for col in cols_standardize]\n","    leave = [(col, None) for col in cols_leave]\n","    x_mapper = DataFrameMapper(leave)\n","\n","    x_train = x_mapper.fit_transform(df_train).astype('float32')\n","    x_val = x_mapper.transform(df_val).astype('float32')\n","    x_test = x_mapper.transform(df_test).astype('float32')\n","\n","    get_target = lambda df: (df['time'].values, df['event'].values)\n","    y_train = get_target(df_train)\n","    y_val = get_target(df_val)\n","    durations_test, events_test = get_target(df_test)\n","    val = x_val, y_val\n","    # We don't need to transform the test labels\n","    durations_test, events_test = get_target(df_test)\n","    durations_train, events_train = get_target(df_train)\n","\n","    in_features = x_train.shape[1]\n"," \n","    out_features = 1\n","    dropout = dropout_rate\n","\n","\n","    net = torch.nn.Sequential(\n","        torch.nn.Linear(in_features, netsize*4),\n","        torch.nn.ReLU(),\n","        torch.nn.BatchNorm1d(netsize*4),\n","        torch.nn.Dropout(dropout),\n","\n","        torch.nn.Linear(netsize*4, netsize),\n","        torch.nn.ReLU(),\n","        torch.nn.BatchNorm1d(netsize),\n","        torch.nn.Dropout(dropout),\n","        \n","        torch.nn.Linear(netsize, out_features)\n","    )\n","\n","\n","    model = CoxPH(net, tt.optim.Adam)\n","    batch_size = n_batch\n","    model.optimizer.set_lr(0.005)\n","    epochs = 500\n","    callbacks = [tt.callbacks.EarlyStopping()]\n","    verbose = True\n","    log = model.fit(x_train, y_train, batch_size, epochs, callbacks, verbose,\n","                val_data=val, val_batch_size=batch_size)\n","\n","\n","    #training c-index\n","    _ = model.compute_baseline_hazards()\n","    surv = model.predict_surv_df(x_train)\n","    ev = EvalSurv(surv, durations_train, events_train, censor_surv='km')\n","    trainci = ev.concordance_td('antolini')\n","    print(\"Train CI:\" + str(trainci))\n","    #testing c-index\n","    surv = model.predict_surv_df(x_test)\n","    ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n","    testci = ev.concordance_td('antolini')\n","    print(\"Test CI:\" + str(testci))\n","\n","    time_grid = np.linspace(0, 3650, 100)\n","    testbs = ev.integrated_brier_score(time_grid) \n","    bsvec = bsvec + np.array(ev.brier_score(time_grid).values)  \n","\n","    resultdf.append((dtype,ind,\n","                     trainci,\n","                    testci,\n","                     testbs\n","                     ))\n","    \n","\n","\n","  return resultdf,bsvec\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gpq-VTu-gVWY"},"source":["#Breast Hyperparameter\n","#Colon Hyperparameter\n","ndata = \"breast\" #cancer type\n","main_folder = '/content/drive/My Drive/deepsurv_DTF/TCGA/'\n","type_folder = main_folder + ndata + \"/\" #change it to the path to deepsurv directory\n","\n","\n","\n","fac_folder = type_folder + 'fac/'\n","ind_folder = type_folder + 'ind/'\n","data_folder = type_folder + ndata + '_clean/'\n","result_folder = main_folder + 'results/'\n","\n","\n","clinical = pd.read_csv(data_folder + ndata + \"_clinic.csv\") #read in clinical data\n","\n","\n","Y = np.array(clinical[\"OS.time\"])\n","E = np.array(clinical[\"OS\"])\n","Age = np.array(clinical[\"agegroup\"])\n","Stage = np.array(clinical[\"stagegroup\"])\n","if ndata != \"breast\": Gender = np.array(clinical[\"gender_n\"])\n","grid = 100\n","interval = 100\n","#Genomic Datasets\n","\n","#single data\n","\n","dtype = \"GE\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","ge = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CN\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","cn = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"DM\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","dm = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CON\"\n","\n","data =pd.concat([ge, cn, dm], axis=1)\n","con = data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-sZZsmAgXai"},"source":["#breast\n","resultdf = []\n","resultname = result_folder + ndata +\"Deepsurv\"+ \"finalcollect.csv\"\n","\n","bsvec = np.zeros(100)\n","resultdf = []\n","datatype = \"GE\"\n","dropout_rate = 0.3\n","netsize = 32\n","n_batch = 128\n","resultdf,bsvec = sinlge_data_experiment_ds(ge,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Deepsurv\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","bsvec = np.zeros(100)\n","\n","datatype = \"DM\"\n","dropout_rate = 0.5\n","netsize = 32\n","n_batch = 128\n","resultdf,bsvec = sinlge_data_experiment_ds(dm,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","bsvec = np.zeros(100)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Deepsurv\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","datatype = \"CN\"\n","dropout_rate = 0.5\n","netsize = 64\n","n_batch = 128\n","resultdf,bsvec = sinlge_data_experiment_ds(cn,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Deepsurv\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","\n","datatype = \"CON\"\n","dropout_rate = 0.5\n","netsize = 64\n","n_batch = 128\n","resultdf,bsvec = sinlge_data_experiment_ds(con,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Deepsurv\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")                                          \n","\n","cols=['DataType','seed','trainCI','testCI','testIBS']\n","results = pd.DataFrame(resultdf,columns = cols)\n","avgtrainci = results['trainCI'].mean()\n","avgtestci = results['testCI'].mean()\n","avgtestibs = results['testIBS'].mean()\n","print(\"Avg TrainCI:\" + str(avgtrainci))\n","print(\"Avg TestCI:\" + str(avgtestci))\n","print(\"Avg TestIBS:\" + str(avgtestibs))\n","results.to_csv(resultname,index=False,header = True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vsnIKif4XKHa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjWJ3oGrgS3b"},"source":["\n","#Colon Hyperparameter\n","ndata = \"colon\"\n","#ndata = \"breast\"\n","main_folder = '/content/drive/My Drive/deepsurv_DTF/TCGA/'\n","type_folder = main_folder + ndata + \"/\" #change it to the path to deepsurv directory\n","\n","\n","\n","fac_folder = type_folder + 'fac/'\n","ind_folder = type_folder + 'ind/'\n","data_folder = type_folder + ndata + '_clean/'\n","result_folder = main_folder + 'results/'\n","\n","\n","clinical = pd.read_csv(data_folder + ndata + \"_clinic.csv\") #read in clinical data\n","\n","\n","Y = np.array(clinical[\"OS.time\"])\n","E = np.array(clinical[\"OS\"])\n","Age = np.array(clinical[\"agegroup\"])\n","Stage = np.array(clinical[\"stagegroup\"])\n","if ndata != \"breast\": Gender = np.array(clinical[\"gender_n\"])\n","grid = 100\n","interval = 100\n","#Genomic Datasets\n","\n","#single data\n","\n","dtype = \"GE\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","ge = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CN\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","cn = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"DM\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","dm = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CON\"\n","\n","data =pd.concat([ge, cn, dm], axis=1)\n","con = data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SBHtUhY3XKJe"},"source":["resultdf = []\n","resultname = result_folder + ndata +\"Deepsurv\"+ \"finalcollect.csv\"\n","\n","\n","bsvec = np.zeros(100)\n","datatype = \"GE\"\n","dropout_rate = 0.5\n","netsize = 128\n","n_batch = 128\n","resultdf,bsvec = sinlge_data_experiment_ds(ge,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Deepsurv\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","\n","bsvec = np.zeros(100)\n","datatype = \"DM\"\n","dropout_rate = 0.3\n","netsize = 128\n","n_batch = 128\n","resultdf,bsvec = sinlge_data_experiment_ds(dm,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Deepsurv\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CN\"\n","dropout_rate = 0.5\n","netsize = 128\n","n_batch = 16\n","resultdf,bsvec = sinlge_data_experiment_ds(cn,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Deepsurv\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CON\"\n","dropout_rate = 0.5\n","netsize = 128\n","n_batch = 128\n","resultdf,bsvec = sinlge_data_experiment_ds(con,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Deepsurv\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","cols=['DataType','seed','trainCI','testCI','testIBS']\n","results = pd.DataFrame(resultdf,columns = cols)\n","results.to_csv(resultname,index=False,header = True)\n","avgtrainci = results['trainCI'].mean()\n","avgtestci = results['testCI'].mean()\n","avgtestibs = results['testIBS'].mean()\n","print(\"Avg TrainCI:\" + str(avgtrainci))\n","print(\"Avg TestCI:\" + str(avgtestci))\n","print(\"Avg TestIBS:\" + str(avgtestibs))\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJPUHMXJK58r","executionInfo":{"status":"ok","timestamp":1627914205094,"user_tz":-480,"elapsed":314,"user":{"displayName":"Jasper Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXkbcEwWi8xY-48do5y_JlYoJ8T1RHzMTLTD80tg=s64","userId":"05397851641183407839"}}},"source":["#Loghazard net"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0sV-UDuBOaH"},"source":["def sinlge_data_experiment_logh(data,dtype,maxind,dropout_rate,netsize,n_batch,resultdf,bsvec):\n","  for ind in np.arange(1,maxind+1): #go through all 20 train/test pairs\n","    np.random.seed(1234)\n","    torchseed = torch.manual_seed(1234)\n","    testind = np.array(pd.read_csv(ind_folder + 'testind' + str(ind)+ \".csv\")[\"x\"]) - 1 # minus 1 to match the array index in Python\n","    trainind = np.array(pd.read_csv(ind_folder + 'trainind' + str(ind)+ \".csv\")[\"x\"]) - 1\n","\n","    print('start with seed' + str(ind))\n","\n","    i = data.shape[1]\n","    df_train = data.loc[trainind,]\n","\n","    df_test = data.loc[testind,]\n","\n","\n","    df_train[\"age\"] = Age[trainind]\n","    df_train[\"stage\"] = Stage[trainind]\n","    df_test[\"age\"] = Age[testind]\n","    df_test[\"stage\"] = Stage[testind]\n","    if ndata != \"breast\": \n","      df_train[\"gender\"] = Gender[trainind]\n","      df_test[\"gender\"] = Gender[testind] \n","\n","    Y_train = Y[trainind]\n","    Y_test = Y[testind]\n","    E_train = E[trainind]\n","    E_test = E[testind]\n","\n","\n","    df_train[\"time\"] = Y_train.astype('float32')\n","    df_train[\"event\"] = E_train.astype('float32')\n","    df_train,df_val = train_test_split(df_train, test_size=0.2)\n","    df_test[\"time\"] = Y_test.astype('float32')\n","    df_test[\"event\"] = E_test.astype('float32')\n","\n","    #cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n","    var_num = i + 2\n","    if ndata != \"breast\": var_num = i + 3\n","    cols_leave = df_train.columns[0:var_num]\n","    print(cols_leave)\n","    #standardize = [([col], StandardScaler()) for col in cols_standardize]\n","    leave = [(col, None) for col in cols_leave]\n","    x_mapper = DataFrameMapper(leave)\n","    x_train = x_mapper.fit_transform(df_train).astype('float32')\n","    x_val = x_mapper.transform(df_val).astype('float32')\n","    x_test = x_mapper.transform(df_test).astype('float32')\n","\n","    num_durations =100\n","    labtrans = LogisticHazard.label_transform(num_durations)\n","    get_target = lambda df: (df['time'].values, df['event'].values)\n","    y_train = labtrans.fit_transform(*get_target(df_train))\n","    y_val = labtrans.transform(*get_target(df_val))\n","\n","\n","    train = (x_train, y_train)\n","    val = (x_val, y_val)    \n","\n","    # We don't need to transform the test labels\n","    durations_test, events_test = get_target(df_test)\n","    durations_train, events_train = get_target(df_train)\n","\n","\n","    in_features = x_train.shape[1]\n","    out_features = labtrans.out_features\n","    batch_norm = True\n","    dropout = dropout_rate\n","    batch_size = n_batch\n","\n","    n_node = netsize\n","    net1 = torch.nn.Sequential(\n","                    torch.nn.Linear(in_features, n_node*4),\n","                    torch.nn.ReLU(),\n","                    torch.nn.BatchNorm1d(n_node*4),\n","                    torch.nn.Dropout(dropout_rate),\n","\n","\n","                    torch.nn.Linear(netsize*4, netsize),\n","                    torch.nn.ReLU(),\n","                    torch.nn.BatchNorm1d(netsize),\n","                    torch.nn.Dropout(dropout_rate),\n","\n","                    torch.nn.Linear(n_node, out_features)\n","                )\n","    model = LogisticHazard(net1, tt.optim.Adam(0.01), duration_index=labtrans.cuts)\n","    epochs = 500\n","    callbacks = [tt.cb.EarlyStopping()]\n","\n","    log = model.fit(x_train, y_train, batch_size, epochs, callbacks, val_data=val)\n","\n","    surv = model.predict_surv_df(x_train)\n","    ev = EvalSurv(surv, durations_train, events_train, censor_surv='km')\n","\n","\n","    time_grid = np.linspace(0, 3650, 100)\n","\n","    trainci = ev.concordance_td('antolini')\n","    print(trainci)\n","    #testing c-index\n","    surv = model.predict_surv_df(x_test)\n","    ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n","\n","    testci = ev.concordance_td('antolini')\n","    testbs = ev.integrated_brier_score(time_grid) \n","    bsvec = bsvec + np.array(ev.brier_score(time_grid).values)  \n","    print(testci)\n","    print(testbs)\n","\n","    resultdf.append((dtype,ind,\n","                     trainci,\n","                    testci,\n","                     testbs\n","                     ))\n","  \n","\n","  return resultdf,bsvec\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"97UrybOITDCL"},"source":["\n","#Colon Hyperparameter\n","ndata = \"colon\"\n","main_folder = '/content/drive/My Drive/deepsurv_DTF/TCGA/'\n","type_folder = main_folder + ndata + \"/\" #change it to the path to deepsurv directory\n","\n","\n","\n","fac_folder = type_folder + 'fac/'\n","ind_folder = type_folder + 'ind/'\n","data_folder = type_folder + ndata + '_clean/'\n","result_folder = main_folder + 'results/'\n","\n","\n","clinical = pd.read_csv(data_folder + ndata + \"_clinic.csv\") #read in clinical data\n","\n","\n","Y = np.array(clinical[\"OS.time\"])\n","E = np.array(clinical[\"OS\"])\n","Age = np.array(clinical[\"agegroup\"])\n","Stage = np.array(clinical[\"stagegroup\"])\n","if ndata != \"breast\": Gender = np.array(clinical[\"gender_n\"])\n","grid = 100\n","interval = 100\n","#Genomic Datasets\n","\n","#single data\n","\n","dtype = \"GE\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","ge = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CN\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","cn = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"DM\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","dm = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CON\"\n","\n","data =pd.concat([ge, cn, dm], axis=1)\n","con = data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ieXvsOv3O2R4"},"source":["resultdf = []\n","resultname = result_folder + ndata +\"LogHazard\"+ \"finalcollect.csv\"\n","\n","\n","bsvec = np.zeros(100)\n","datatype = \"GE\"\n","dropout_rate = 0.1\n","netsize = 32\n","n_batch = 32\n","resultdf,bsvec = sinlge_data_experiment_logh(ge,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Loghazard\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","\n","bsvec = np.zeros(100)\n","datatype = \"DM\"\n","dropout_rate = 0.1\n","netsize = 32\n","n_batch = 16\n","resultdf,bsvec = sinlge_data_experiment_logh(dm,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Loghazard\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CN\"\n","dropout_rate = 0.1\n","netsize = 32\n","n_batch = 32\n","resultdf,bsvec = sinlge_data_experiment_logh(cn,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Loghazard\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CON\"\n","dropout_rate = 0.1\n","netsize = 32\n","n_batch = 16\n","resultdf,bsvec = sinlge_data_experiment_logh(con,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Loghazard\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","cols=['DataType','seed','trainCI','testCI','testIBS']\n","results = pd.DataFrame(resultdf,columns = cols)\n","results.to_csv(resultname,index=False,header = True)\n","avgtrainci = results['trainCI'].mean()\n","avgtestci = results['testCI'].mean()\n","avgtestibs = results['testIBS'].mean()\n","print(\"Avg TrainCI:\" + str(avgtrainci))\n","print(\"Avg TestCI:\" + str(avgtestci))\n","print(\"Avg TestIBS:\" + str(avgtestibs))\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGwEf8zXRyFE"},"source":["\n","ndata = \"breast\"\n","main_folder = '/content/drive/My Drive/deepsurv_DTF/TCGA/'\n","type_folder = main_folder + ndata + \"/\" #change it to the path to deepsurv directory\n","\n","\n","\n","fac_folder = type_folder + 'fac/'\n","ind_folder = type_folder + 'ind/'\n","data_folder = type_folder + ndata + '_clean/'\n","result_folder = main_folder + 'results/'\n","\n","\n","clinical = pd.read_csv(data_folder + ndata + \"_clinic.csv\") #read in clinical data\n","\n","\n","Y = np.array(clinical[\"OS.time\"])\n","E = np.array(clinical[\"OS\"])\n","Age = np.array(clinical[\"agegroup\"])\n","Stage = np.array(clinical[\"stagegroup\"])\n","if ndata != \"breast\": Gender = np.array(clinical[\"gender_n\"])\n","grid = 100\n","interval = 100\n","#Genomic Datasets\n","\n","#single data\n","\n","dtype = \"GE\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","ge = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CN\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","cn = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"DM\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","dm = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CON\"\n","\n","data =pd.concat([ge, cn, dm], axis=1)\n","con = data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtIOCq0ZRyOg"},"source":["resultdf = []\n","resultname = result_folder + ndata +\"LogHazard\"+ \"finalcollect.csv\"\n","\n","\n","bsvec = np.zeros(100)\n","datatype = \"GE\"\n","dropout_rate = 0.3\n","netsize = 16\n","n_batch = 32\n","resultdf,bsvec = sinlge_data_experiment_logh(ge,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Loghazard\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","\n","bsvec = np.zeros(100)\n","datatype = \"DM\"\n","dropout_rate = 0.1\n","netsize = 16\n","n_batch = 32\n","resultdf,bsvec = sinlge_data_experiment_logh(dm,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Loghazard\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CN\"\n","dropout_rate = 0.1\n","netsize = 16\n","n_batch = 16\n","resultdf,bsvec = sinlge_data_experiment_logh(cn,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Loghazard\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CON\"\n","dropout_rate = 0.1\n","netsize = 32\n","n_batch = 16\n","resultdf,bsvec = sinlge_data_experiment_logh(con,datatype,20,dropout_rate,netsize,n_batch,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"Loghazard\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","cols=['DataType','seed','trainCI','testCI','testIBS']\n","results = pd.DataFrame(resultdf,columns = cols)\n","results.to_csv(resultname,index=False,header = True)\n","avgtrainci = results['trainCI'].mean()\n","avgtestci = results['testCI'].mean()\n","avgtestibs = results['testIBS'].mean()\n","print(\"Avg TrainCI:\" + str(avgtrainci))\n","print(\"Avg TestCI:\" + str(avgtestci))\n","print(\"Avg TestIBS:\" + str(avgtestibs))\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBOhLht0AaJ4"},"source":["#DTS Encoded Survival Net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OvmmpcwVTonl"},"source":["def sinlge_data_experiment_dtsm(data,dtype,maxind,lw,es,en,ns,n_batch,resultdf,bsvec):\n","  for ind in np.arange(1,maxind+1): #go through all 20 train/test pairs\n","    np.random.seed(1234)\n","    torchseed = torch.manual_seed(1234)\n","    testind = np.array(pd.read_csv(ind_folder + 'testind' + str(ind)+ \".csv\")[\"x\"]) - 1 # minus 1 to match the array index in Python\n","    trainind = np.array(pd.read_csv(ind_folder + 'trainind' + str(ind)+ \".csv\")[\"x\"]) - 1\n","\n","    print('start with seed' + str(ind))\n","\n","    i = data.shape[1]\n","    df_train = data.loc[trainind,]\n","\n","    df_test = data.loc[testind,]\n","\n","\n","    df_train[\"age\"] = Age[trainind]\n","    df_train[\"stage\"] = Stage[trainind]\n","    df_test[\"age\"] = Age[testind]\n","    df_test[\"stage\"] = Stage[testind]\n","    if ndata != \"breast\": \n","      df_train[\"gender\"] = Gender[trainind]\n","      df_test[\"gender\"] = Gender[testind] \n","\n","    Y_train = Y[trainind]\n","    Y_test = Y[testind]\n","    E_train = E[trainind]\n","    E_test = E[testind]\n","\n","\n","    df_train[\"time\"] = Y_train.astype('float32')\n","    df_train[\"event\"] = E_train.astype('float32')\n","    df_train,df_val = train_test_split(df_train, test_size=0.2)\n","    df_test[\"time\"] = Y_test.astype('float32')\n","    df_test[\"event\"] = E_test.astype('float32')\n","\n","    #cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n","    var_num = i + 2\n","    if ndata != \"breast\": var_num = i + 3\n","    cols_leave = df_train.columns[0:var_num]\n","    print(cols_leave)\n","    #standardize = [([col], StandardScaler()) for col in cols_standardize]\n","    leave = [(col, None) for col in cols_leave]\n","    x_mapper = DataFrameMapper(leave)\n","    x_train = x_mapper.fit_transform(df_train).astype('float32')\n","    x_val = x_mapper.transform(df_val).astype('float32')\n","    x_test = x_mapper.transform(df_test).astype('float32')\n","\n","    num_durations = 100\n","    labtrans = LogisticHazard.label_transform(num_durations)\n","    get_target = lambda df: (df['time'].values, df['event'].values)\n","    y_train_surv = labtrans.fit_transform(*get_target(df_train))\n","    y_val_surv = labtrans.transform(*get_target(df_val))\n","\n","    train = tt.tuplefy(x_train, (y_train_surv, x_train))\n","    val = tt.tuplefy(x_val, (y_val_surv, x_val))\n","\n","    # We don't need to transform the test labels\n","    durations_test, events_test = get_target(df_test)\n","    durations_train, events_train = get_target(df_train)\n","    \n","    class NetAESurv(nn.Module):\n","      def __init__(self, in_features, encoded_features, out_features):\n","        super().__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(in_features, es*3), nn.ReLU(),\n","            nn.Linear(es*3, es*2), nn.ReLU(),\n","            nn.Linear(es*2, es*2), nn.ReLU(),\n","            nn.Linear(es*2, es*2), nn.ReLU(),\n","    \n","            nn.Linear(es*2, encoded_features),\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.Linear(encoded_features, es*2), nn.ReLU(),\n","            nn.Linear(es*2, es*2), nn.ReLU(),\n","            nn.Linear(es*2, es*2), nn.ReLU(),\n","            nn.Linear(es*2, es*3), nn.ReLU(),\n","            nn.Linear(es*3, in_features),\n","        )\n","        self.surv_net = nn.Sequential(\n","            nn.Linear(encoded_features, ns*3), nn.ReLU(),\n","            nn.Linear(ns*3, ns*3), nn.ReLU(),\n","          #  nn.Dropout(0.5),\n","            nn.Linear(ns*3, ns*2), nn.ReLU(),\n","            nn.Linear(ns*2, ns*2), nn.ReLU(),\n","         #   nn.Dropout(0.5),\n","            nn.Linear(ns*2, ns), nn.ReLU(),\n","            nn.Linear(ns, out_features),\n","        )\n","\n","      def forward(self, input):\n","        encoded = self.encoder(input)\n","        decoded = self.decoder(encoded)\n","        phi = self.surv_net(encoded)\n","        return phi, decoded\n","\n","      def predict(self, input):\n","        # Will be used by model.predict later.\n","        # As this only has the survival output, \n","        # we don't have to change LogisticHazard.\n","        encoded = self.encoder(input)\n","        return self.surv_net(encoded)\n","\n","    in_features = x_train.shape[1]\n","    encoded_features = en\n","    out_features = labtrans.out_features\n","    net = NetAESurv(in_features, encoded_features, out_features)\n","\n","    class LossAELogHaz(nn.Module):\n","      def __init__(self, alpha):\n","        super().__init__()\n","        assert (alpha >= 0) and (alpha <= 1), 'Need `alpha` in [0, 1].'\n","        self.alpha = alpha\n","        self.loss_surv = NLLLogistiHazardLoss() #NLLPMFLoss() #\n","        self.loss_ae = nn.MSELoss()\n","        \n","      def forward(self, phi, decoded, target_loghaz, target_ae):\n","        idx_durations, events = target_loghaz\n","        loss_surv = self.loss_surv(phi, idx_durations, events)\n","        loss_ae = self.loss_ae(decoded, target_ae)\n","        return self.alpha * loss_surv + (1 - self.alpha) * loss_ae\n","\n","    loss = LossAELogHaz(lw)\n","    model = LogisticHazard(net, tt.optim.Adam(0.01), duration_index=labtrans.cuts, loss=loss)\n","\n","    metrics = dict(\n","    loss_surv = LossAELogHaz(1),\n","    loss_ae   = LossAELogHaz(0)\n","    )\n","    callbacks = [tt.cb.EarlyStopping()]\n","\n","    batch_size = n_batch\n","    epochs = 100\n","    model.fit(*train, batch_size, epochs, callbacks, val_data=val, metrics=metrics)\n","\n","    #training c-index\n","    surv = model.predict_surv_df(x_train)\n","    ev = EvalSurv(surv, durations_train, events_train, censor_surv='km')\n","\n","\n","    trainci = ev.concordance_td()\n","    print(trainci)\n","    #testing c-index\n","    surv = model.predict_surv_df(x_test)\n","    ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n","\n","    testci = ev.concordance_td()\n","    time_grid = np.linspace(0, 3650, 100)\n","\n","    testbs = ev.integrated_brier_score(time_grid)\n","\n","    bsvec = bsvec + np.array(ev.brier_score(time_grid).values)  \n","    \n","    print(testci)\n","\n","    resultdf.append((dtype,ind,\n","                    trainci,\n","                    testci,\n","                    testbs))\n","\n","\n","  return resultdf,bsvec\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ci40LJFKTori"},"source":["\n","#Colon Hyperparameter\n","ndata = \"colon\"\n","main_folder = '/content/drive/My Drive/deepsurv_DTF/TCGA/'\n","type_folder = main_folder + ndata + \"/\" #change it to the path to deepsurv directory\n","\n","\n","\n","fac_folder = type_folder + 'fac/'\n","ind_folder = type_folder + 'ind/'\n","data_folder = type_folder + ndata + '_clean/'\n","result_folder = main_folder + 'results/'\n","\n","\n","clinical = pd.read_csv(data_folder + ndata + \"_clinic.csv\") #read in clinical data\n","\n","\n","Y = np.array(clinical[\"OS.time\"])\n","E = np.array(clinical[\"OS\"])\n","Age = np.array(clinical[\"agegroup\"])\n","Stage = np.array(clinical[\"stagegroup\"])\n","if ndata != \"breast\": Gender = np.array(clinical[\"gender_n\"])\n","grid = 100\n","interval = 100\n","#Genomic Datasets\n","\n","#single data\n","\n","dtype = \"GE\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","ge = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CN\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","cn = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"DM\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","dm = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CON\"\n","\n","data =pd.concat([ge, cn, dm], axis=1)\n","con = data\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rL7kDBzZDhw"},"source":["#colon\n","resultdf = []\n","resultname = result_folder + ndata +\"DTSM\"+ \"finalcollect.csv\"\n","\n","bsvec = np.zeros(100)\n","datatype = \"GE\"\n","batlist = 128\n","lossweight = 0.3 \n","encodenum = 16\n","encodesize = 128\n","netsize = 64\n","\n","\n","resultdf,bsvec = sinlge_data_experiment_dtsm(ge,datatype,20,lossweight,encodesize,encodenum,netsize,batlist,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"DTSM\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","\n","bsvec = np.zeros(100)\n","datatype = \"DM\"\n","batlist = 16\n","lossweight = 0.3 \n","encodenum = 16\n","encodesize = 128\n","netsize = 64\n","\n","\n","resultdf,bsvec = sinlge_data_experiment_dtsm(dm,datatype,20,lossweight,encodesize,encodenum,netsize,batlist,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"DTSM\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CN\"\n","batlist = 16\n","lossweight = 0.3 \n","encodenum = 16\n","encodesize = 128\n","netsize = 64\n","\n","\n","resultdf,bsvec = sinlge_data_experiment_dtsm(cn,datatype,20,lossweight,encodesize,encodenum,netsize,batlist,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"DTSM\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CON\"\n","batlist = 128\n","lossweight = 0.3 \n","encodenum = 16\n","encodesize = 128\n","netsize = 64\n","\n","\n","resultdf,bsvec = sinlge_data_experiment_dtsm(con,datatype,20,lossweight,encodesize,encodenum,netsize,batlist,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"DTSM\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","\n","cols=['DataType','seed','trainCI','testCI','testIBS']\n","results = pd.DataFrame(resultdf,columns = cols)\n","results.to_csv(resultname,index=False,header = True)\n","avgtrainci = results['trainCI'].mean()\n","avgtestci = results['testCI'].mean()\n","avgtestibs = results['testIBS'].mean()\n","print(\"Avg TrainCI:\" + str(avgtrainci))\n","print(\"Avg TestCI:\" + str(avgtestci))\n","print(\"Avg TestIBS:\" + str(avgtestibs))\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aY4gDm-LTouc"},"source":["#breast\n","#Breast Hyperparameter\n","\n","ndata = \"breast\"\n","main_folder = '/content/drive/My Drive/deepsurv_DTF/TCGA/'\n","type_folder = main_folder + ndata + \"/\" #change it to the path to deepsurv directory\n","\n","\n","\n","fac_folder = type_folder + 'fac/'\n","ind_folder = type_folder + 'ind/'\n","data_folder = type_folder + ndata + '_clean/'\n","result_folder = main_folder + 'results/'\n","\n","\n","clinical = pd.read_csv(data_folder + ndata + \"_clinic.csv\") #read in clinical data\n","\n","\n","Y = np.array(clinical[\"OS.time\"])\n","E = np.array(clinical[\"OS\"])\n","Age = np.array(clinical[\"agegroup\"])\n","Stage = np.array(clinical[\"stagegroup\"])\n","if ndata != \"breast\": Gender = np.array(clinical[\"gender_n\"])\n","grid = 100\n","interval = 100\n","#Genomic Datasets\n","\n","#single data\n","\n","dtype = \"GE\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","ge = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CN\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","cn = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"DM\"\n","data = pd.read_csv(data_folder + dtype + \"_\" + ndata + \"_t.csv\") #read in single Genomic data type\n","dm = data.drop(data.columns[0], axis=1)\n","\n","dtype = \"CON\"\n","\n","data =pd.concat([ge, cn, dm], axis=1)\n","con = data\n","\n","resultdf = []\n","resultname = result_folder + ndata +\"DTSM\"+ \"finalcollect.csv\"\n","\n","bsvec = np.zeros(100)\n","datatype = \"GE\"\n","batlist = 16\n","lossweight = 0.5 \n","encodenum = 16\n","encodesize = 128\n","netsize = 64\n","\n","\n","resultdf,bsvec = sinlge_data_experiment_dtsm(ge,datatype,20,lossweight,encodesize,encodenum,netsize,batlist,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"DTSM\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","\n","bsvec = np.zeros(100)\n","datatype = \"DM\"\n","batlist = 128\n","lossweight = 0.3 \n","encodenum = 16\n","encodesize = 128\n","netsize = 64\n","\n","\n","resultdf,bsvec = sinlge_data_experiment_dtsm(dm,datatype,20,lossweight,encodesize,encodenum,netsize,batlist,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"DTSM\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CN\"\n","batlist = 16\n","lossweight = 0.3 \n","encodenum = 16\n","encodesize = 128\n","netsize = 64\n","\n","\n","resultdf,bsvec = sinlge_data_experiment_dtsm(cn,datatype,20,lossweight,encodesize,encodenum,netsize,batlist,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"DTSM\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","bsvec = np.zeros(100)\n","datatype = \"CON\"\n","batlist = 16\n","lossweight = 0.5 \n","encodenum = 16\n","encodesize = 128\n","netsize = 64\n","\n","\n","resultdf,bsvec = sinlge_data_experiment_dtsm(con,datatype,20,lossweight,encodesize,encodenum,netsize,batlist,resultdf,bsvec)\n","#results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + datatype +\"DTSM\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n","\n","\n","cols=['DataType','seed','trainCI','testCI','testIBS']\n","results = pd.DataFrame(resultdf,columns = cols)\n","results.to_csv(resultname,index=False,header = True)\n","avgtrainci = results['trainCI'].mean()\n","avgtestci = results['testCI'].mean()\n","avgtestibs = results['testIBS'].mean()\n","print(\"Avg TrainCI:\" + str(avgtrainci))\n","print(\"Avg TestCI:\" + str(avgtestci))\n","print(\"Avg TestIBS:\" + str(avgtestibs))\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cwm0IiKmToxE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AfsLgxGDTo0L"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VBfgqO6To2p"},"source":["#DTS TF-Encoded Survival net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVpV0jNtS0UJ"},"source":["#colon\n","ndata = \"colon\"\n","lossweight = 0.5\n","encodenum = 16\n","encodesize = 16\n","netsize = 128\n","batch = 32\n","\n","toprank = 25"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGJjvgwoT6E2"},"source":["#breast\n","ndata = \"breast\"\n","lossweight = 0.7\n","encodenum = 8\n","encodesize = 64\n","netsize = 16\n","batch = 32\n","\n","toprank = 17"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jzQI68eT8l0"},"source":["main_folder = '/content/drive/My Drive/deepsurv_DTF/TCGA/'\n","type_folder = main_folder + ndata + \"/\" #change it to the path to deepsurv directory\n","\n","\n","\n","fac_folder = type_folder + 'fac/'\n","ind_folder = type_folder + 'ind/'\n","data_folder = type_folder + ndata + '_clean/'\n","result_folder = main_folder + 'results/'\n","\n","\n","clinical = pd.read_csv(data_folder + ndata + \"_clinic.csv\") #read in clinical data\n","\n","\n","Y = np.array(clinical[\"OS.time\"])\n","E = np.array(clinical[\"OS\"])\n","Age = np.array(clinical[\"agegroup\"])\n","Stage = np.array(clinical[\"stagegroup\"])\n","if ndata != \"breast\": Gender = np.array(clinical[\"gender_n\"])\n","grid = 100\n","interval = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BicmUVe7fVSn"},"source":["# Final model\n","#given hyperparameters of final model and make predictions\n","avglist = []\n","resultdf = []\n","bsvec = np.zeros(100)\n","resultname = result_folder + ndata + 'DTSM' + \".csv\"\n","\n","\n","i = toprank\n","\n","for ind in np.arange(1,21): #go through all 20 train/test pairs\n","  np.random.seed(1234)\n","  _ = torch.manual_seed(1234)\n","  lw = lossweight\n","  en = encodenum\n","  ns = netsize\n","  es = encodesize\n","\n","  ba = batch\n","\n","  print('seed:' + str(ind))\n","  print('rank: ' + str(i))\n","\n","  df_train = pd.read_csv(fac_folder + 'train' + str(ind) + \"rank\" + str(i) + \".csv\",header = None)\n","  df_test = pd.read_csv(fac_folder + 'test' + str(ind) + \"rank\"+ str(i) + \".csv\",header= None)\n","  testind = np.array(pd.read_csv(ind_folder + 'testind' + str(ind)+ \".csv\")[\"x\"]) - 1 # minus 1 to match the array index in Python\n","  trainind = np.array(pd.read_csv(ind_folder + 'trainind' + str(ind)+ \".csv\")[\"x\"]) - 1\n","\n","  df_train[\"age\"] = Age[trainind]\n","  df_train[\"stage\"] = Stage[trainind]\n","  if ndata != \"breast\":df_train[\"gender\"] = Gender[trainind]\n","  df_test[\"age\"] = Age[testind]\n","  df_test[\"stage\"] = Stage[testind]\n","  if ndata != \"breast\":df_test[\"gender\"] = Gender[testind]\n","\n","  Y_train = Y[trainind]\n","  Y_test = Y[testind]\n","  E_train = E[trainind]\n","  E_test = E[testind]\n","\n","  df_train[\"time\"] = Y_train.astype('float32')\n","  df_train[\"event\"] = E_train.astype('float32')\n","  df_train,df_val = train_test_split(df_train, test_size=0.2)\n","  df_test[\"time\"] = Y_test.astype('float32')\n","  df_test[\"event\"] = E_test.astype('float32')\n","\n","  #cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n","  var_num = i + 2\n","  if ndata != \"breast\": var_num = i + 3\n","  cols_leave = df_train.columns[0:var_num]\n","  print(cols_leave)\n","  #standardize = [([col], StandardScaler()) for col in cols_standardize]\n","  leave = [(col, None) for col in cols_leave]\n","  x_mapper = DataFrameMapper(leave)\n","  x_train = x_mapper.fit_transform(df_train).astype('float32')\n","  x_val = x_mapper.transform(df_val).astype('float32')\n","  x_test = x_mapper.transform(df_test).astype('float32')\n","\n","  num_durations = interval\n","  labtrans = LogisticHazard.label_transform(num_durations)\n","  get_target = lambda df: (df['time'].values, df['event'].values)\n","  y_train_surv = labtrans.fit_transform(*get_target(df_train))\n","  y_val_surv = labtrans.transform(*get_target(df_val))\n","\n","  train = tt.tuplefy(x_train, (y_train_surv, x_train))\n","  val = tt.tuplefy(x_val, (y_val_surv, x_val))\n","\n","  # We don't need to transform the test labels\n","  durations_test, events_test = get_target(df_test)\n","  durations_train, events_train = get_target(df_train)\n","  \n","  class NetAESurv(nn.Module):\n","    def __init__(self, in_features, encoded_features, out_features):\n","      super().__init__()\n","      self.encoder = nn.Sequential(\n","          nn.Linear(in_features, es*3), nn.ReLU(),\n","          nn.Linear(es*3, es*2), nn.ReLU(),\n","          nn.Linear(es*2, es*2), nn.ReLU(),\n","          nn.Linear(es*2, es*2), nn.ReLU(),\n","  \n","          nn.Linear(es*2, encoded_features),\n","      )\n","      self.decoder = nn.Sequential(\n","          nn.Linear(encoded_features, es*2), nn.ReLU(),\n","          nn.Linear(es*2, es*2), nn.ReLU(),\n","          nn.Linear(es*2, es*2), nn.ReLU(),\n","          nn.Linear(es*2, es*3), nn.ReLU(),\n","          nn.Linear(es*3, in_features),\n","      )\n","      self.surv_net = nn.Sequential(\n","          nn.Linear(encoded_features, ns*3), nn.ReLU(),\n","          nn.Linear(ns*3, ns*3), nn.ReLU(),\n","          nn.Linear(ns*3, ns*2), nn.ReLU(),\n","          nn.Linear(ns*2, ns*2), nn.ReLU(),\n","          nn.Linear(ns*2, ns), nn.ReLU(),\n","          nn.Linear(ns, out_features),\n","      )\n","\n","    def forward(self, input):\n","      encoded = self.encoder(input)\n","      decoded = self.decoder(encoded)\n","      phi = self.surv_net(encoded)\n","      return phi, decoded\n","\n","    def predict(self, input):\n","      # Will be used by model.predict later.\n","      # As this only has the survival output, \n","      # we don't have to change LogisticHazard.\n","      encoded = self.encoder(input)\n","      return self.surv_net(encoded)\n","\n","  in_features = x_train.shape[1]\n","  encoded_features = en\n","  out_features = labtrans.out_features\n","  net = NetAESurv(in_features, encoded_features, out_features)\n","\n","  class LossAELogHaz(nn.Module):\n","    def __init__(self, alpha):\n","      super().__init__()\n","      assert (alpha >= 0) and (alpha <= 1), 'Need `alpha` in [0, 1].'\n","      self.alpha = alpha\n","      self.loss_surv = NLLLogistiHazardLoss() #NLLPMFLoss() #\n","      self.loss_ae = nn.MSELoss()\n","      \n","    def forward(self, phi, decoded, target_loghaz, target_ae):\n","      idx_durations, events = target_loghaz\n","      loss_surv = self.loss_surv(phi, idx_durations, events)\n","      loss_ae = self.loss_ae(decoded, target_ae)\n","      return self.alpha * loss_surv + (1 - self.alpha) * loss_ae\n","\n","  loss = LossAELogHaz(lw)\n","  model = LogisticHazard(net, tt.optim.Adam(0.01), duration_index=labtrans.cuts, loss=loss)\n","\n","  metrics = dict(\n","  loss_surv = LossAELogHaz(1),\n","  loss_ae   = LossAELogHaz(0)\n","  )\n","  callbacks = [tt.cb.EarlyStopping()]\n","\n","  batch_size = ba\n","  epochs = 500\n","  model.fit(*train, batch_size, epochs, callbacks, val_data=val, metrics=metrics)\n","\n","  #training c-index\n","  surv = model.predict_surv_df(x_train)\n","  ev = EvalSurv(surv, durations_train, events_train, censor_surv='km')\n","\n","  time_grid = np.linspace(durations_test.min(), durations_test.max(), grid)\n","\n","  trainci = ev.concordance_td()\n","  print(trainci)\n","  #testing c-index\n","  surv = model.predict_surv_df(x_test)\n","  ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n","\n","  testci = ev.concordance_td()\n","\n","  #time_grid = np.linspace(durations_test.min(), durations_test.max(), grid)\n","  time_grid = np.linspace(0, 3650, 100)\n","\n","  testbs = ev.integrated_brier_score(time_grid)\n","  bsvec = bsvec + np.array(ev.brier_score(time_grid).values)\n","  \n","  print(testci)\n","\n","  resultdf.append((ind,i,\n","                  trainci,\n","                  testci,\n","                  testbs))\n","\n","\n","  cols=['seed','rank','trainCI','testCI','testIBS']\n","\n","  results = pd.DataFrame(resultdf,columns = cols)\n","\n","\n","avgtrainci = results['trainCI'].mean()\n","avgtestci = results['testCI'].mean()\n","avgtestibs = results['testIBS'].mean()\n","print(\"Avg TrainCI:\" + str(avgtrainci))\n","print(\"Avg TestCI:\" + str(avgtrainci))\n","print(\"Avg TestIBS:\" + str(avgtestibs))\n","\n","results.to_csv(resultname,index=False,header = True)\n","avgbs = bsvec/20\n","np.savetxt(result_folder + ndata + \"DTSM\"+ \"BSovertime.csv\", avgbs, delimiter=\",\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3b62N-BM7xdA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORx_yu2C7xeu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_PVp_84OFH5f"},"source":["#TF-LogisticHazard\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7eH_WDJKmYac"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# For preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn_pandas import DataFrameMapper \n","import torch # For building the networks \n","from torch import nn\n","import torch.nn.functional as F\n","import torchtuples as tt # Some useful functions\n","from pycox.datasets import metabric\n","from pycox.models import LogisticHazard\n","from pycox.models.loss import NLLLogistiHazardLoss, CoxPHLoss, NLLPMFLoss,NLLPCHazardLoss\n","from pycox.evaluation import EvalSurv\n","from lifelines.utils import concordance_index\n","from lifelines import CoxPHFitter\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","\n","#set randomseed for both torch and numpy\n","#np.random.seed(1234)\n","#_ = torch.manual_seed(1234)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_e0O66Nl8Vd"},"source":["#colon\n","ndata = \"colon\"\n","dropout = 0.3\n","batchsize = 32\n","nodenum = 16\n","toprank = 46"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MgwztkLl_fn"},"source":["#breast\n","ndata = \"breast\"\n","dropout = 0.1\n","batchsize = 32\n","nodenum = 64\n","toprank = 22"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ii41hCUvl_h2"},"source":["\n","main_folder = '/content/drive/My Drive/deepsurv_DTF/TCGA/'\n","type_folder = main_folder + ndata + \"/\" #change it to the path to deepsurv directory\n","\n","\n","fac_folder = type_folder + 'fac/'\n","ind_folder = type_folder + 'ind/'\n","data_folder = type_folder + ndata + '_clean/'\n","result_folder = main_folder + 'results/'\n","\n","\n","clinical = pd.read_csv(data_folder + ndata + \"_clinic.csv\") #read in clinical data\n","\n","\n","Y = np.array(clinical[\"OS.time\"])\n","E = np.array(clinical[\"OS\"])\n","Age = np.array(clinical[\"agegroup\"])\n","Stage = np.array(clinical[\"stagegroup\"])\n","if ndata != \"breast\": Gender = np.array(clinical[\"gender_n\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9b0CQiel_kA"},"source":["avglist = []\n","resultdf = []\n","bsvec = np.zeros(100)\n","resultname = result_folder + ndata + 'LogHazard' + \".csv\"\n","interval = 100\n","grid = 100\n","droprate = dropout\n","n_batch = batchsize\n","n_node = nodenum\n","i = toprank\n","\n","\n","for ind in np.arange(1,21): #go through all 20 train/test pairs\n","  np.random.seed(1234)\n","  torchseed = torch.manual_seed(1234)\n","\n","\n","  print('seed:' + str(ind))\n","  print('rank: ' + str(i))\n","\n","  df_train = pd.read_csv(fac_folder + 'train' + str(ind) + \"rank\" + str(i) + \".csv\",header = None)\n","  df_test = pd.read_csv(fac_folder + 'test' + str(ind) + \"rank\"+ str(i) + \".csv\",header= None)\n","  testind = np.array(pd.read_csv(ind_folder + 'testind' + str(ind)+ \".csv\")[\"x\"]) - 1 # minus 1 to match the array index in Python\n","  trainind = np.array(pd.read_csv(ind_folder + 'trainind' + str(ind)+ \".csv\")[\"x\"]) - 1\n","\n","  df_train[\"age\"] = Age[trainind]\n","  df_train[\"stage\"] = Stage[trainind]\n","  if ndata != \"breast\": df_train[\"gender\"] = Gender[trainind]\n","  df_test[\"age\"] = Age[testind]\n","  df_test[\"stage\"] = Stage[testind]\n","  if ndata != \"breast\": df_test[\"gender\"] = Gender[testind]\n","\n","  Y_train = Y[trainind]\n","  Y_test = Y[testind]\n","  E_train = E[trainind]\n","  E_test = E[testind]\n","\n","  df_train[\"time\"] = Y_train.astype('float32')\n","  df_train[\"event\"] = E_train.astype('float32')\n","  df_train,df_val = train_test_split(df_train, test_size=0.2)\n","  df_test[\"time\"] = Y_test.astype('float32')\n","  df_test[\"event\"] = E_test.astype('float32')\n","\n","  #cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n","  var_num = i + 2\n","  if ndata != \"breast\": var_num = i + 3\n","  cols_leave = df_train.columns[0:var_num]\n","  print(cols_leave)\n","  #standardize = [([col], StandardScaler()) for col in cols_standardize]\n","  leave = [(col, None) for col in cols_leave]\n","  x_mapper = DataFrameMapper(leave)\n","  x_train = x_mapper.fit_transform(df_train).astype('float32')\n","  x_val = x_mapper.transform(df_val).astype('float32')\n","  x_test = x_mapper.transform(df_test).astype('float32')\n","\n","  num_durations = interval\n","  labtrans = LogisticHazard.label_transform(num_durations)\n","  get_target = lambda df: (df['time'].values, df['event'].values)\n","  y_train = labtrans.fit_transform(*get_target(df_train))\n","  y_val = labtrans.transform(*get_target(df_val))\n","\n","\n","  train = (x_train, y_train)\n","  val = (x_val, y_val)    \n","\n","  # We don't need to transform the test labels\n","  durations_test, events_test = get_target(df_test)\n","  durations_train, events_train = get_target(df_train)\n","  \n","\n","  in_features = x_train.shape[1]\n","  out_features = labtrans.out_features\n","\n","\n","  net2 = torch.nn.Sequential(\n","      torch.nn.Linear(in_features, n_node),\n","      torch.nn.ReLU(),\n","      torch.nn.BatchNorm1d(n_node),\n","      torch.nn.Dropout(droprate),\n","      \n","      torch.nn.Linear(n_node, n_node),\n","      torch.nn.ReLU(),\n","      torch.nn.BatchNorm1d(n_node),\n","      torch.nn.Dropout(droprate),\n","      \n","      torch.nn.Linear(n_node, out_features)\n","  )\n","\n","  model = LogisticHazard(net2, tt.optim.Adam(0.01), duration_index=labtrans.cuts)\n","\n","\n","  batch_size = n_batch\n","  epochs = 500\n","  callbacks = [tt.cb.EarlyStopping()]\n","\n","  log = model.fit(x_train, y_train, batch_size, epochs, callbacks, val_data=val)\n","\n","  surv = model.predict_surv_df(x_train)\n","  ev = EvalSurv(surv, durations_train, events_train, censor_surv='km')\n","  \n","\n","\n","  time_grid = np.linspace(durations_test.min(), durations_test.max(), grid)\n","\n","  trainci = ev.concordance_td()\n","  print(trainci)\n","  #testing c-index\n","  surv = model.predict_surv_df(x_test)\n","  ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n","\n","  testci = ev.concordance_td()\n","\n","   #time_grid = np.linspace(durations_test.min(), durations_test.max(), grid)\n","  time_grid = np.linspace(0, 3650, 100)\n","\n","  testbs = ev.integrated_brier_score(time_grid)\n","  bsvec = bsvec + np.array(ev.brier_score(time_grid).values)\n","  \n","  print(testci)\n","\n","  resultdf.append((ind,i,\n","                  trainci,\n","                  testci,\n","                  testbs))\n","\n","\n","  cols=['seed','rank','trainCI','testCI','testIBS']\n","\n","  results = pd.DataFrame(resultdf,columns = cols)\n","\n","\n","avgtrainci = results['trainCI'].mean()\n","avgtestci = results['testCI'].mean()\n","avgtestibs = results['testIBS'].mean()\n","print(\"Avg TrainCI:\" + str(avgtrainci))\n","print(\"Avg TestCI:\" + str(avgtrainci))\n","print(\"Avg TestIBS:\" + str(avgtestibs))\n","\n","results.to_csv(resultname,index=False,header = True)\n","np.savetxt(result_folder + ndata + \"LogHazard\"+ \"BSovertime.csv\", bsvec/20, delimiter=\",\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZRfxAHhel_nE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbdBqDP5l_pC"},"source":["#TF-Deepsurv\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YqniZFHxXzr"},"source":["\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# For preprocessing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn_pandas import DataFrameMapper \n","\n","import torch # For building the networks \n","from torch import nn\n","import torch.nn.functional as F\n","import torchtuples as tt # Some useful functions\n","\n","from pycox.datasets import metabric\n","from pycox.models import LogisticHazard\n","from pycox.models.loss import NLLLogistiHazardLoss\n","from pycox.evaluation import EvalSurv\n","from pycox.datasets import metabric\n","from pycox.models import CoxPH\n","from pycox.evaluation import EvalSurv\n","\n","import tensorflow as tf\n","\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","## Survival Analysis using Keras\n","from keras.initializers import glorot_uniform\n","from keras.models import Model\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout\n","from keras.optimizers import SGD, RMSprop\n","from keras.regularizers import l2\n","import keras.backend as K\n","from lifelines.utils import concordance_index\n","from lifelines import CoxPHFitter\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","np.random.seed(1234)  # for reproducibility\n","_ = torch.manual_seed(1234)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bRNNssHxbxJ"},"source":["#colon\n","ndata = \"colon\"\n","dropout = 0.3\n","batchsize = 32\n","nodenum = 16\n","toprank = 18"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RF67kX1kxeHa"},"source":["#breast\n","ndata = \"breast\"\n","dropout = 0.3\n","batchsize = 128\n","nodenum = 16\n","toprank = 22"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hAS1jYmyxZqr"},"source":["\n","\n","main_folder = '/content/drive/My Drive/deepsurv_DTF/TCGA/'\n","type_folder = main_folder + ndata + \"/\" #change it to the path to deepsurv directory\n","\n","\n","fac_folder = type_folder + 'fac/'\n","ind_folder = type_folder + 'ind/'\n","data_folder = type_folder + ndata + '_clean/'\n","result_folder = main_folder + 'results/'\n","\n","\n","clinical = pd.read_csv(data_folder + ndata + \"_clinic.csv\") #read in clinical data\n","\n","\n","Y = np.array(clinical[\"OS.time\"])\n","E = np.array(clinical[\"OS\"])\n","Age = np.array(clinical[\"agegroup\"])\n","Stage = np.array(clinical[\"stagegroup\"])\n","if ndata != \"breast\": Gender = np.array(clinical[\"gender_n\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D4HfCREpxrry"},"source":["\n","#given hyperparameters of final model and make predictions\n","avglist = []\n","resultdf = []\n","bsvec = np.zeros(100)\n","resultname = result_folder + ndata + 'Deepsurv' + \".csv\"\n","\n","i = toprank\n","n_node = nodenum\n","droprate = dropout\n","n_batch = batchsize\n","\n","\n","for ind in np.arange(1,21): #go through all 20 train/test pairs\n","  np.random.seed(1234)\n","  torchseed = torch.manual_seed(1234)\n","\n","  print('start with seed' + str(ind))\n","\n","  print('current rank: ' + str(i))\n","  df_train = pd.read_csv(fac_folder + 'train' + str(ind) + \"rank\" + str(i) + \".csv\",header = None)\n","  df_test = pd.read_csv(fac_folder + 'test' + str(ind) + \"rank\"+ str(i) + \".csv\",header= None)\n","  \n","  testind = np.array(pd.read_csv(ind_folder + 'testind' + str(ind)+ \".csv\")[\"x\"]) - 1 # minus 1 to match the array index in Python\n","  trainind = np.array(pd.read_csv(ind_folder + 'trainind' + str(ind)+ \".csv\")[\"x\"]) - 1\n","\n","  df_train[\"age\"] = Age[trainind]\n","  df_train[\"stage\"] = Stage[trainind]\n","  if ndata != \"breast\":df_train[\"gender\"] = Gender[trainind]\n","  df_test[\"age\"] = Age[testind]\n","  df_test[\"stage\"] = Stage[testind]\n","  if ndata != \"breast\":df_test[\"gender\"] = Gender[testind] \n","\n","\n","  Y_train = Y[trainind]\n","  Y_test = Y[testind]\n","  E_train = E[trainind]\n","  E_test = E[testind]\n","\n","\n","  df_train[\"time\"] = Y_train.astype('float32')\n","  df_train[\"event\"] = E_train.astype('float32')\n","  df_train,df_val = train_test_split(df_train, test_size=0.2)\n","  df_test[\"time\"] = Y_test.astype('float32')\n","  df_test[\"event\"] = E_test.astype('float32')\n","  #step2\n","\n","  #cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n","  var_num = i + 2\n","  if ndata != \"breast\": var_num = i + 3\n","  cols_leave = df_train.columns[0:var_num]\n","  print(cols_leave)\n","  #standardize = [([col], StandardScaler()) for col in cols_standardize]\n","  leave = [(col, None) for col in cols_leave]\n","  x_mapper = DataFrameMapper(leave)\n","\n","  x_train = x_mapper.fit_transform(df_train).astype('float32')\n","  x_val = x_mapper.transform(df_val).astype('float32')\n","  x_test = x_mapper.transform(df_test).astype('float32')\n","\n","  get_target = lambda df: (df['time'].values, df['event'].values)\n","  y_train = get_target(df_train)\n","  y_val = get_target(df_val)\n","  durations_test, events_test = get_target(df_test)\n","  val = x_val, y_val\n","  # We don't need to transform the test labels\n","  durations_test, events_test = get_target(df_test)\n","  durations_train, events_train = get_target(df_train)\n","\n","  in_features = x_train.shape[1]\n","  out_features = 1\n","  dropout = droprate\n","\n","\n","  net = torch.nn.Sequential(\n","      torch.nn.Linear(in_features, n_node*4),\n","      torch.nn.ReLU(),\n","      torch.nn.BatchNorm1d(n_node*4),\n","      torch.nn.Dropout(dropout),\n","\n","      torch.nn.Linear(n_node*4, n_node*2),\n","      torch.nn.ReLU(),\n","      torch.nn.BatchNorm1d(n_node*2),\n","      torch.nn.Dropout(dropout),\n","\n","      torch.nn.Linear(n_node*2, n_node*2),\n","      torch.nn.ReLU(),\n","      torch.nn.BatchNorm1d(n_node*2),\n","      torch.nn.Dropout(dropout),\n","      \n","      torch.nn.Linear(n_node*2, out_features)\n","  )\n","\n","\n","  model = CoxPH(net, tt.optim.Adam)\n","  batch_size = n_batch\n","  model.optimizer.set_lr(0.005)\n","  epochs = 500\n","  callbacks = [tt.callbacks.EarlyStopping()]\n","  verbose = True\n","  log = model.fit(x_train, y_train, batch_size, epochs, callbacks, verbose,\n","              val_data=val, val_batch_size=batch_size)\n","\n","\n","\n","  #training c-index\n","  _ = model.compute_baseline_hazards()\n","  surv = model.predict_surv_df(x_train)\n","  ev = EvalSurv(surv, durations_train, events_train, censor_surv='km')\n","  trainci = ev.concordance_td('antolini')\n","  print(trainci)\n","  #testing c-index\n","  surv = model.predict_surv_df(x_test)\n","  ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n","  testci = ev.concordance_td('antolini')\n","  print(testci)\n","\n","\n","  #time_grid = np.linspace(durations_test.min(), durations_test.max(), grid)\n","  time_grid = np.linspace(0, 3650, 100)\n","\n","  testbs = ev.integrated_brier_score(time_grid)\n","  bsvec = bsvec + np.array(ev.brier_score(time_grid).values)\n","  \n","  print(testci)\n","\n","  resultdf.append((ind,i,\n","                  trainci,\n","                  testci,\n","                  testbs))\n","\n","\n","  cols=['seed','rank','trainCI','testCI','testIBS']\n","\n","  results = pd.DataFrame(resultdf,columns = cols)\n","\n","\n","avgtrainci = results['trainCI'].mean()\n","avgtestci = results['testCI'].mean()\n","avgtestibs = results['testIBS'].mean()\n","print(\"Avg TrainCI:\" + str(avgtrainci))\n","print(\"Avg TestCI:\" + str(avgtrainci))\n","print(\"Avg TestIBS:\" + str(avgtestibs))\n","\n","results.to_csv(resultname,index=False,header = True)\n","np.savetxt(result_folder + ndata + \"Deepsurv\"+ \"BSovertime.csv\", bsvec/20, delimiter=\",\")\n"],"execution_count":null,"outputs":[]}]}